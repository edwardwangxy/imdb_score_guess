---
title: "Score Guessing with imdb reviews"
author: "Xiangyu Wang"
date: "Dec 1st, 2016"
---
#### based on data: https://www.kaggle.com/deepmatrix/imdb-5000-movie-dataset
#### [Shiny dashboard version](https://uwwxy.shinyapps.io/guess_movie_score_imdb/)

### Start with loading require libraries
***

```{r }
library(rvest)
library(RColorBrewer)
library(SnowballC)
library(tm)
library(wordcloud)
library(XML)
library(RColorBrewer)
library(rpart.plot)
```

***

### Start picking movies from dataset 
***
1. setting minimum reviews for picking movies. 
2. subset "score","link","title","number of reviews" from the dataset larger than minimum reviews. 
3. split into 8 different score dataset. 


```{r }
set_review_num = 200 
imdb_data <- read.csv("movie_metadata.csv")
#order the scores
imdb_data_select <- imdb_data[order(-imdb_data$imdb_score),c("imdb_score","movie_imdb_link","movie_title","num_user_for_reviews")]
imdb_data_select <- subset(imdb_data_select, num_user_for_reviews >= set_review_num) #subset dataset
#pb <- txtProgressBar(0,9, style = 3) #create progress bar
for(i in 2:9) #start spliting movies into different scores.
{
  var_name = paste("imdb_score_",i,sep = "")
  imdb_data_score <- subset(imdb_data_select, imdb_score >=i)
  imdb_data_score <- subset(imdb_data_score, imdb_score <i+1)
  imdb_data_score$movie_imdb_link <- lapply(imdb_data_score$movie_imdb_link, as.character)
  assign(var_name, imdb_data_score)
  #  setTxtProgressBar(pb, i)
  #  Sys.sleep(0.5)
}
#close(pb)
```

***

### Start crawling reviews from imdb website
***
1. Load functions and read each score dataset.
2. Based on the setting of # of movies and # of pages start crawling reviews movie by movie and page by page.
3. Save each movie's reviews in each row and the score for the movie at the end for future use.
4. Combine all training reviews into a large training dataset.
5. Generate a training term frequency table with scores and save all term names into list.
6. Removed all temporary objects created and save rawdata objects into files for future load to save time.
+(For a faster generating speed, I will only grab 1 movie for each score and 1 page for each movie)

```{r }
dict_save_location <- "dictionary" #save location of rawdata and term tables
source("imdb_review_scraping_func.R")
source("imdb_class_tree_func.R") #read classify function to create table for raw data
#choose inputs here
reviews_each_movie = 10
max_movies_pick = 10
total_movie_count = 0
####
#progress_bar_counting = 0
for(n in 2:9)
{
  link_list_name = paste("imdb_score_",n,sep="")
  total_movie_count = total_movie_count + min(nrow(get(link_list_name)),max_movies_pick)
}
#pb2 <- txtProgressBar(min = 0, max = total_movie_count, char = "=", style = 3)
raw_save_list = c(NULL)
for(n in 2:9)
{
  link_list_name = paste("imdb_score_",n,sep="")
  for(i in 1:min(nrow(get(link_list_name)),max_movies_pick)) #do not achieve more than available movies
  {
#    progress_bar_counting = progress_bar_counting + 1
    if(i ==1) #create empty list for later use
    {
      review_final = list(NULL)
      review_score = list(NULL)
    }
    review_test = myimdb.reviews.full(get(link_list_name)$movie_imdb_link[[i]], max_reviews = reviews_each_movie)
#    setTxtProgressBar(pb2, progress_bar_counting)
    review_score = c(review_test, get(link_list_name)$imdb_score[i])
    review_final = rbind(review_final, review_score)
  }
  review_name = paste("score_",n,"_reviews", sep = "")
  raw_save_list = c(raw_save_list, review_name, link_list_name)
  review_final = review_final[-1,] #get rid of NULL row
  assign(review_name, review_final)
  #rm(list = c(link_list_name)) 
}
#close(pb2)
total = c(NULL)
#pb5 <- txtProgressBar(min = 0, max = 8, char = "=", style = 3)
for(i in 2:9)
{
  score_review_name = paste("score_",i,"_reviews", sep = "")
  total = rbind(total, get(score_review_name))
#  setTxtProgressBar(pb5, i)
} #input all score review objects names as list into "score_review_name_list"
#and row combind reviews of each movie with scores into "total"
#close(pb5)
total_clean_reviews = imdb_score_clean_func(total[,-ncol(total)]) #cleanup all reviews
total_table = imdb_score_term_func(total_clean_reviews, K=100) #achieve 100 terms for all reviews
all_variables = total_table[,1] #achieve only all the terms' name into a list

training_table = imdb_train_data_generate(all_variables, total, processbar = FALSE) #use the function to create a training data table
training_table$imdb_score = as.factor(training_table$imdb_score) #change numeric into factor for classification
rawdata_name <- sprintf("rawdata-%d-%d.Rda",max_movies_pick,reviews_each_movie)
raw_save_list = c(raw_save_list,"max_movies_pick","all_variables","training_table")
save(list = raw_save_list, file=sprintf("%s/rawdata/%s", dict_save_location,rawdata_name)) #save rawdatas into files
rm(list = ls()[-grep(paste(raw_save_list,collapse="|"), ls())])  #remove all the useless object
```

***

### Start creating term tables
***
1. Load rawdata saved from above, this step is optional.
   +(if already run above two steps, loading process could be omited).
2. Load functions and set a K_input to limit the # of terms in the term tables.
3. Create term tables with K terms for different scores and save all objects to dictionary for future use to save time.
4. Generate wordclouds for each score reviews
5. Remove all useless objects to save memory.
+(For more accurate I will load a larger rawdata with 30 movies for each score and 10 pages for each movie instead of using the data grabed above)

```{r }
load("dictionary/rawdata/rawdata-20-1000.Rda") #using this function to load data directly
source("imdb_score_clean_func.R")
source("imdb_score_term_func.R")

K_input = 50

dictionary_name <- sprintf("dict-%d-%d.Rda",max_movies_pick,K_input)
dict_save_location <- "dictionary"
par(mfrow = c(2,2))
#pb3 <- txtProgressBar(min = 0, max = 9, char = "=", style = 3)
objects_name_to_save = c(NULL)
for(i in 2:9)
{
  review_name = paste("score_",i,"_reviews",sep = "")
  term_name = paste("score_",i,"_term",sep = "")
  clean_reviews = imdb_score_clean_func(get(review_name)[,-2])
  table = imdb_score_term_func(clean_reviews, K=K_input)
  assign(term_name, table)
  #myfunc.wordcloud(clean_reviews, remove_words = c(stopwords("english"),"film","movie","can","films","movies","will","scenes","just","one","like"))
  objects_name_to_save = c(objects_name_to_save,review_name, term_name)
#  setTxtProgressBar(pb3, i)
}
#close(pb3)
objects_name_to_save = c(objects_name_to_save,"all_variables","training_table")
save(list = objects_name_to_save, file=sprintf("%s/%s", dict_save_location, dictionary_name))
rm(list = ls()[-grep(paste(objects_name_to_save,collapse="|"), ls())])
```

***

### Start Guessing using term tables
***
1. Load dictionary saved from above, this step is optional.
    +(if already run above step, loading process could be omited).
2. Load all the functions needed.
3. Set the url of the movie picked to guess the score. Currently using Movie <Memento>'s url. The real score is 8.5
    +(for faster processing time, I will only grab 10 pages for guessing)
4. Set pages of reviews need to grab for testing.
5. Grab reviews from imdb website into a list.
6. Start compare all terms with the term table and calculate the expected value for each score.
7. Order and pick the first two result as the first and second guess.

```{r }
#load("dictionary/dict-30-70-50.Rda") #using this function to load dictionary directly
source("imdb_guess_review_func.R")
source("imdb_review_scraping_func.R")
test_web_url = "http://www.imdb.com/title/tt0209144/?ref_=fn_al_tt_1" #<Memento> imdb_url
max_reviews_test = 1000
review_prob = c(NULL)
try_score_review = myimdb.reviews.full(test_web_url, max_reviews_test)

#pb4 <- txtProgressBar(min = 0, max = 9, char = "=", style = 3)
for(i in 2:9)
{
  term_name = paste("score_",i,"_term",sep="")
  review_term_table_name = paste("review_",i,"_table",sep="")
  review_term_table = guess_imdb_review_score(try_score_review, get(term_name))
  review_prob = c(review_prob,sum(as.numeric(review_term_table[,2])))
  assign(review_term_table_name, review_term_table)
#  setTxtProgressBar(pb4, i)
}
#close(pb4)
guess_table = cbind(2:9,review_prob)
```

The following bar plot is comparing testing movie's terms in each score term table

```{r }
par(mfrow = c(2,2))
barplot(as.numeric(review_2_table[1:10,3]), legend.text = review_2_table[1:10,1],col=brewer.pal(10, "Paired"))
barplot(as.numeric(review_3_table[1:10,3]), legend.text = review_3_table[1:10,1],col=brewer.pal(10, "Paired"))
barplot(as.numeric(review_4_table[1:10,3]), legend.text = review_4_table[1:10,1],col=brewer.pal(10, "Paired"))
barplot(as.numeric(review_5_table[1:10,3]), legend.text = review_5_table[1:10,1],col=brewer.pal(10, "Paired"))
barplot(as.numeric(review_6_table[1:10,3]), legend.text = review_6_table[1:10,1],col=brewer.pal(10, "Paired"))
barplot(as.numeric(review_7_table[1:10,3]), legend.text = review_7_table[1:10,1],col=brewer.pal(10, "Paired"))
barplot(as.numeric(review_8_table[1:10,3]), legend.text = review_8_table[1:10,1],col=brewer.pal(10, "Paired"))
barplot(as.numeric(review_9_table[1:10,3]), legend.text = review_9_table[1:10,1],col=brewer.pal(10, "Paired"))
par(mfrow = c(1,1))
colnames(guess_table)=c("score","prob")
guess_table <- guess_table[order(guess_table[,2],decreasing=TRUE),]
guess_table
#rm(list = ls()[-grep("guess_table", ls())])
cat(paste("First Guess is score ", guess_table[1,1],"\nSecond Guess is score ", guess_table[2,1]))
```

***

### Start creating classification tree and analysis
***
1. Load all the functions needed.
2. Use the test review dataset above and create a test term frequency table using training term names.
3. Use the training table create a classification tree and plot it.
4. Pruned the tree and plot the pruned tree.
5. Predict movie scores using two trees.

```{r }
source("imdb_class_tree_func.R") #read classify function to create table for raw data
test_table = imdb_test_data_generate(all_variables, try_score_review, processbar = FALSE) #use the function to create a training data table

#create classification trees
fit <- rpart(imdb_score ~ .,
             method="class", data=training_table,
             control = rpart.control(minsplit=20, cp=0.001))

#plot the tree
prp(fit, main="Classification Tree",
    #extra="auto", # display prob of survival and percent of obs
    fallen.leaves=TRUE, # put the leaves on the bottom of the page
    shadow.col="gray", # shadows under the leaves
    branch.lty=2, # draw branches using dotted lines
    #branch=.5, # change angle of branch lines
    faclen=0, # faclen=0 to print full factor names
    trace=1, # print the automatically calculated cex
    split.cex=1.2, # make the split text larger than the node text
    split.prefix="Is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    #col=cols, border.col=cols, # red if spam, blue if not
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=.5) # round the split box corners a tad

#prune the tree
pfit<- prune(fit, cp=fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])
#plot the pruned tree
prp(pfit, main="Pruned Tree",
    #extra="auto", # display prob of survival and percent of obs
    fallen.leaves=TRUE, # put the leaves on the bottom of the page
    shadow.col="gray", # shadows under the leaves
    branch.lty=2, # draw branches using dotted lines
    #branch=.5, # change angle of branch lines
    faclen=0, # faclen=0 to print full factor names
    trace=1, # print the automatically calculated cex
    split.cex=1.2, # make the split text larger than the node text
    split.prefix="Is ", # put "is " before split text
    split.suffix="?", # put "?" after split text
    #col=cols, border.col=cols, # red if spam, blue if not
    split.box.col="lightgray", # lightgray split boxes (default is white)
    split.border.col="darkgray", # darkgray border on split boxes
    split.round=.5) # round the split box corners a tad


#using the training dataset retest the trees, these function could be used to see the difference
conf.matrix <- table(training_table$imdb_score, predict(fit,type="class"))
accuracy_training_ori = sum(diag(conf.matrix))/sum(conf.matrix)
conf.matrix2 <- table(training_table$imdb_score, predict(pfit,type="class"))
accuracy_training_pruned = sum(diag(conf.matrix2))/sum(conf.matrix2)

predict = predict(fit, test_table, type="class")
cat(paste("Predict with original tree is: ",as.character(predict), "\nthe accuracy is: ", as.character(round(accuracy_training_ori*100, 2)), "%",sep = ""))
predict2 = predict(pfit, test_table, type="class")
cat(paste("Predict with original tree is: ",as.character(predict2), "\nthe accuracy is: ", as.character(round(accuracy_training_pruned*100, 2)), "%",sep = ""))
```

***

### Try random forest analysis 
***
1. Create a random forest using training table.
2. Predict the test table with the random forest.

```{r }
library(randomForest)
rownames(training_table) = NULL
ffit <- randomForest(imdb_score ~ .,
                     ntree=2000,
                     data=training_table)
cat(paste("Guess using the random forest is: ", as.character(predict(ffit, test_table, type="response")), sep = ""))
print(fit) # view results 
importance(ffit)[1:10,]
```

***

### The End
***

---
title: "main_markdown.R"
author: "edwardwangxy"
date: "Tue Dec  6 15:53:35 2016"
---
